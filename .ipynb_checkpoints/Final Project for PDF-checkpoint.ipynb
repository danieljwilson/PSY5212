{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Background\" data-toc-modified-id=\"Background-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Background</a></span></li><li><span><a href=\"#Process\" data-toc-modified-id=\"Process-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Process</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mapping-Eccentricity\" data-toc-modified-id=\"Mapping-Eccentricity-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Mapping Eccentricity</a></span></li><li><span><a href=\"#ROI-Masking-Based-on-Eccentricity\" data-toc-modified-id=\"ROI-Masking-Based-on-Eccentricity-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>ROI Masking Based on Eccentricity</a></span></li><li><span><a href=\"#Accuracy-Calculation\" data-toc-modified-id=\"Accuracy-Calculation-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Accuracy Calculation</a></span></li></ul></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#All-Subject-Data\" data-toc-modified-id=\"All-Subject-Data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>All Subject Data</a></span></li><li><span><a href=\"#Plots\" data-toc-modified-id=\"Plots-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Plots</a></span></li><li><span><a href=\"#V1---V4-Accuracies\" data-toc-modified-id=\"V1---V4-Accuracies-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>V1 - V4 Accuracies</a></span></li><li><span><a href=\"#Higher-Level-Visual-Region-Accuracies\" data-toc-modified-id=\"Higher-Level-Visual-Region-Accuracies-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Higher Level Visual Region Accuracies</a></span></li></ul></li><li><span><a href=\"#Analysis\" data-toc-modified-id=\"Analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Effect-Size\" data-toc-modified-id=\"Effect-Size-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Effect Size</a></span></li><li><span><a href=\"#Spatial-Arrangement\" data-toc-modified-id=\"Spatial-Arrangement-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Spatial Arrangement</a></span></li></ul></li><li><span><a href=\"#Discussion\" data-toc-modified-id=\"Discussion-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**PSY 5212: Final Project**\n",
    "\n",
    "Amin Banihashemi, Hause Lin & Daniel J Wilson \n",
    "\n",
    "---\n",
    "\n",
    "*Note: If possible, please view this via the Binder link. This will allow the interactive running of the Jupyter notebook (you do not need to download anything), so you can recreate plots to explore the data yourself. Just click the link \"launch binder\" below. Note that it will take a few minutes to launch (it is building a custom Python environment using Docker).*\n",
    "\n",
    "[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/danieljwilson/PSY5212/master)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our eye has a wide range of vision, the 1.5mm wide fovea is by far the most sensitive area, where fine detail and color may be distinguished (\"Fovea centralis,\" 2018). \n",
    "\n",
    "The central fovea in the image below (Figure 1) is shown to cover 5 degrees, however we found general agreement that the foveal region could be assumed to cover 2 degrees - roughly the size of a thumb held at arms length (Fairchild, 2013). We used this 2 degree number to define the fovea for our project.\n",
    "\n",
    "---\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Peripheral_vision.svg/499px-Peripheral_vision.svg.png'>\n",
    "\n",
    "*Figure 1.* Peripheral vision of the human eye\n",
    "\n",
    "---\n",
    "\n",
    "The goal of our project was to look at areas of the visual regions that responded differentially to foveal and non-foveal (peripheral) stimuli, and examine how these areas, when analyzed on their own, might predict the scene categories used in the experiment (faces, objects, scenes, scrambled).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Eccentricity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the visual cortex is retinotopically organized means that adjacent neurons tend to have similar receptive fields. This can be extended to the voxel level, leading to the idea that each voxel, and the neurons it contains, will have at least somewhat similar receptive field properties. This is referred to as a population receptive field, or pRF (Victor et al., 1994).\n",
    "\n",
    "We can calculate the pRF size based on an understanding of how the pRF size influences the fMRI time course (Wandell and Doumoulin, 2008). We can also calculate eccentricity, a measurement of distance from a central fixation poitn, as there is a systematic relationship between duty cycle and visual eccentricity (Smith et al., 2001). \n",
    "\n",
    "We computed pRFs for all three subjects in matlab using the `runPRFanalysis.m` script, written by Dirk Bernhardt-Walther.\n",
    "\n",
    "---\n",
    "\n",
    "**Converting Eccentricity to Visual Angle**\n",
    "\n",
    "This step first required entering the measurements from the experimental setup, in our case these were:\n",
    "\n",
    "    stim_distance_cm = 228.3;\n",
    "\n",
    "    stim_diameter_cm = 27.8;\n",
    "\n",
    "    stim_diameter_pixels = 768;\n",
    "    \n",
    "With these data we were able to run the following commands in matlab to get the eccentricity of each PRF in degrees:\n",
    "\n",
    "1) Look at the eccentricity results for the voxels of a given ROI\n",
    "```matlab\n",
    "ROI_eccentricity = results.ecc(roiIdx);\n",
    "```\n",
    "2) Calculate the eccentricity in centimeters\n",
    "```matlab\n",
    "ecc_cm = ROI_eccentricity / stim_diameter_pixels * stim_diameter_cm;\n",
    "```\n",
    "3) Confirm that the eccentricity value in cm is valid (i.e. the value is equal to or smaller than the actual size of the stimulus)\n",
    "```matlab\n",
    "ecc_valid = (ecc_cm <= stim_diameter_cm);\n",
    "```\n",
    "4) Use the inverse tangent function calculate the degrees of eccentricity (distance from the center of the stimulus).\n",
    "```matlab\n",
    "ecc_degrees = atand(ecc_cm/stim_distance_cm);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI Masking Based on Eccentricity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With eccentricity converted to visual angle, we used AFNI's `3dcalc` function to separate voxels representing the two eccentricity categories of interest.\n",
    "\n",
    "To create a mask isolating the eccentricities of 2 degrees and below we ran the following code:\n",
    "\n",
    "```afni\n",
    "3dcalc -a ../masks/0003_V1_clean_mask+orig -b 0003_PRFresults_bucket+orig[6] -expr 'step(a) * step(b)*step(2-b)' -prefix 0003_V1_fov_mask\n",
    "```\n",
    "\n",
    "Where `a` is the original ROI mask and `b` is the eccentricity results in degrees for the whole brain. The `step` function then used the math `2-b` to remove any voxels from the original ROI mask which were greater than 2.\n",
    "\n",
    "The opposite math, `b-2` was used to isolate the non-foveal eccentricities (angles greater than 2 degrees), with the following code:\n",
    "\n",
    "```afni\n",
    "3dcalc -a ../masks/0003_V1_clean_mask+orig -b 0003_PRFresults_bucket+orig[6] -expr 'step(a) * step(b)*step(b-2)' -prefix 0003_V1_non_fov_mask\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Calculation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracies were calculated in Matlab.\n",
    "\n",
    "For classification we used `libsvm`, and the raw data, as in a previous assignment this combination had given the highest classification accuracy.\n",
    "\n",
    "We ran the following code in Matlab in order to obtain classfication accuracy and a confusion matrix:\n",
    "\n",
    "1) Select a subject (e.g. `0003`), foveal/peripheral ROI (e.g. `V1_fov`).\n",
    "```matlab\n",
    "dsetsV1_fov = loadROIdata('0003','V1_fov','raw');\n",
    "```\n",
    "2) Check classification accuracy values and create a confusion matrix (see Figure 2) to visualize where errors are being made.\n",
    "```matlab\n",
    "[accV1_fov, cmV1_fov]=classifyFPO(dsetsV1_fov);\n",
    "```\n",
    "This was repeated for all ROIs for all subjects at both foveal (less than 2 degrees) and peripheral (greater than 2 degrees) eccentricities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "![title](data/ConfusionMatrix.png)\n",
    "\n",
    "*Figure 2.* Sample confusion matrix output. The diagonal values are used to calculate the accuracy of predictions. Each diagonal value was divided by 8, which is the total number of predictions, and expressed as percentage to yield the accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a summary of all data. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotly.tools.set_credentials_file(username='dapper2733', api_key='sbnoWN1I8CGjjK5MzXae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROIs = ['V1', 'V2', 'V3', 'V4', 'OPA','PPA','RSC','FFA','LOC']\n",
    "categories = ['Face', 'Object', 'Place', 'Scrambled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Subject Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"http://www.danieljwilson.com/data/Accuracies.csv\")\n",
    "\n",
    "table = ff.create_table(df)\n",
    "\n",
    "plotly.offline.iplot(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 3*. Accuracy for each subject, for each category, for each region, for both foveal and peripheral pRFs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create mean of subjects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Subject = 5\n",
    "\n",
    "d = []\n",
    "\n",
    "for category in categories:\n",
    "    for foveal in range(0,2):\n",
    "        for ROI in ROIs:\n",
    "        \n",
    "            mean = (float(df.Accuracy[(df.Subject==2) & (df.Foveal==foveal) & (df.Region == ROI) & (df.Category==category)]) + float(df.Accuracy[(df.Subject==3) & (df.Foveal==foveal) & (df.Region == ROI) & (df.Category==category)]) + float(df.Accuracy[(df.Subject==4) & (df.Foveal==foveal) & (df.Region == ROI) & (df.Category==category)]))/3\n",
    "\n",
    "            d.append({'Subject': Subject, 'Category': category, 'Region': ROI, 'Foveal': foveal, 'Accuracy': round(mean, 2)})\n",
    "\n",
    "d = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Add subject 5 (mean of subjects 2,3,4) to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Examine data by subject and region**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plotly.graph_objs import *\n",
    "\n",
    "# Plotting function\n",
    "\n",
    "def ROI_accuracies(Subject: int, Category: str) -> None:\n",
    "    \"\"\"\n",
    "    Draws a plotly plot based on Subject and stimulus Category\n",
    "    \"\"\"\n",
    "    trace_foveal = Bar(x=df.Region[(df.Subject==Subject) & (df.Foveal==1) & (df.Category==Category)],\n",
    "                      y=df.Accuracy[(df.Subject==Subject) & (df.Foveal==1) & (df.Category==Category)],\n",
    "                      name='Foveal',\n",
    "                      marker=dict(color='#59606D'))\n",
    "\n",
    "    trace_non_foveal = Bar(x=df.Region[(df.Subject==Subject) & (df.Foveal==0) & (df.Category==Category)],\n",
    "                    y=df.Accuracy[(df.Subject==Subject) & (df.Foveal==0) & (df.Category==Category)],\n",
    "                    name='Non-Foveal',\n",
    "                    marker=dict(color='#A2D5F2'))\n",
    "\n",
    "    data = [trace_foveal, trace_non_foveal]\n",
    "\n",
    "    layout = Layout(title=\"Accuracy for Subject \" + str(Subject) + \": \" + str(Category),\n",
    "                    xaxis=dict(title='Accuracy'),\n",
    "                    yaxis=dict(title='ROI'))\n",
    "    \n",
    "    fig = Figure(data=data, layout=layout)\n",
    "\n",
    "    plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Interactive Plot**\n",
    "\n",
    "To dynamically view any subject and category you like, just change the `Subject` and `Category` variables in the function call below (`ROI_accuracies`).\n",
    "\n",
    "Subject: {2, 3, 4, 5} *subject 5 = mean of 2, 3 & 4* \n",
    "\n",
    "Category: {'Face', 'Object', 'Place', 'Scrambled'}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_accuracies(3, 'Face')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 4*. Foveal and Peripheral categorization accuracy for all regions for a selected sibject and category.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Examine data across subject by category and region**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# Plotting function\n",
    "\n",
    "def fov_vs_peripheral(Region: str, Category: str) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Plot change from foveal to peripheral for all subjects as well as a mean\n",
    "    line using plotly.\n",
    "    \"\"\"\n",
    "    \n",
    "    sub2_y1 = float(df.Accuracy[(df.Subject==2) & (df.Foveal==1) & (df.Category==Category) & (df.Region==Region)])\n",
    "    sub2_y2 = float(df.Accuracy[(df.Subject==2) & (df.Foveal==0) & (df.Category==Category) & (df.Region==Region)])\n",
    "    sub3_y1 = float(df.Accuracy[(df.Subject==3) & (df.Foveal==1) & (df.Category==Category) & (df.Region==Region)])\n",
    "    sub3_y2 = float(df.Accuracy[(df.Subject==3) & (df.Foveal==0) & (df.Category==Category) & (df.Region==Region)])\n",
    "    sub4_y1 = float(df.Accuracy[(df.Subject==4) & (df.Foveal==1) & (df.Category==Category) & (df.Region==Region)])\n",
    "    sub4_y2 = float(df.Accuracy[(df.Subject==4) & (df.Foveal==0) & (df.Category==Category) & (df.Region==Region)])\n",
    "\n",
    "    # Create traces\n",
    "    subject_2 = go.Scatter(\n",
    "        x = ['Foveal', 'Non-Foveal'],\n",
    "        y = [sub2_y1,\n",
    "             sub2_y2],\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Subj. 2'\n",
    "    )\n",
    "    subject_3 = go.Scatter(\n",
    "        x = ['Foveal', 'Non-Foveal'],\n",
    "        y = [sub3_y1,\n",
    "             sub3_y2],\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Subj. 3'\n",
    "    )\n",
    "    subject_4 = go.Scatter(\n",
    "        x = ['Foveal', 'Non-Foveal'],\n",
    "        y = [sub4_y1,\n",
    "             sub4_y2],\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Subj. 4'\n",
    "    )\n",
    "    subject_mean = go.Scatter(\n",
    "        x = ['Foveal', 'Non-Foveal'],\n",
    "        y = [(sub2_y1 + sub3_y1 + sub4_y1)/3,\n",
    "             (sub2_y2 + sub3_y2 + sub4_y2)/3],\n",
    "        mode = 'lines+markers',\n",
    "        name = 'Mean'\n",
    "    )\n",
    "\n",
    "    data = [subject_2, subject_3, subject_4, subject_mean]\n",
    "\n",
    "    plotly.offline.iplot({\n",
    "        \"data\": data,\n",
    "        \"layout\": Layout(title=\"Foveal vs. Non-Foveal Accuracy: \" + str(Region) + \", \" + str(Category))\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Interactive Plot**\n",
    "\n",
    "To dynamically view differences in accuracy for all subjects between foveal and peripheral pRFs just change the `Region` and `Category` variables in the function call below (`fov_vs_peripheral`).\n",
    "\n",
    "Region: {'V1', 'V2', 'V3', 'V4', 'FFA', 'LOC', 'OPA', 'PPA', 'RSC'}\n",
    "\n",
    "Category: {'Face', 'Object', 'Place', 'Scrambled'}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fov_vs_peripheral('RSC', 'Face')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 5*. Foveal vs Peripheral categorization accuracy for all subjects for a given ROI and category.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 - V4 Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/Lower.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 6*. Average of the three subjects for each ROI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher Level Visual Region Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/Higher.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 7*. Average of the three subjects for each ROI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect Size\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculations of effect sizes for each category, in each ROI, when comparing foveal to non-foveal accuracy.\n",
    "\n",
    "Note that non-foveal is **subtracted** from foveal, so positive values mean that the foveal subregions were more accurate in categorizing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/EffectSize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 8*. Bold arrow in Face: effect sizes: The FFA had 100% accuracy in all threes subjects for face and as such an effect size could not be calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Arrangement\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were curious to see whether there might be a clear difference in the arrangement of the ROIs in terms of their receptivity at different eccentricities. \n",
    "\n",
    "To look at this in a very rough way we loaded the filters for the foveal and peripheral ROIs into AFNI after running the following code in AFNI to combine but separately color the two eccentricities:\n",
    "\n",
    "```afni\n",
    "3dcalc -a 0004_OPA_fov_mask+orig -b 0004_OPA_non_fov_mask+orig -expr 'a+2*b' -prefix 0004_OPA_both_eccs\n",
    "```\n",
    "\n",
    "This use of `3dcalc` employed the simple math `a + 2*b` to give the foveal mask a value of 1 and the non-foveal mask a value of 2 for a given ROI. The thought going in was that the lower visual areas (V1-V4) might be more continguously arranged in terms of eccentricity.\n",
    "\n",
    "Looking below we found this to be somewhat the case in, for example, V1 and PPA, just based on visual inspection (subject 4).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/V1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 9*. V1 for subject 4. The two colors represent foveal and peripheral pRFs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PPA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/PPA.png)\n",
    "\n",
    "*Figure 10*. PPA for subject 4. The two colors represent foveal and peripheral pRFs. Additional ROI images for subject 4 can be found in the `data` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "However, we also looked at cluster number for each region to see if that gave us more analytically robust information. The idea being that if we set the cluster size to `2` then we would be able to see how many separate clustered regions there were at each eccentricity level. If a given eccentricity was completely contiguous then that would give a cluster number of just 2. \n",
    "\n",
    "Below are screenshots of the cluster analysis tool for V1 foveal and non-foveal masks (subject 4).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V1 Foveal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/Clusters.png\" align = \"left\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 11*. Cluster information for V1 foveal. There are a total of 15 clusters (each consisting of 2 or more voxels). There were also 24 single voxels (unclustered), so a total of 15 clusters + 24 single voxels for region V1 foveal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V1 Non-Foveal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/Clusters2.png\" align = \"left\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 12*. Cluster information for V1 peripheral. There are a total of 8 clusters and 34 single voxels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All Regions, Cluster Number**\n",
    "\n",
    "Using this technique we can look at the remaining ROIs (Subject 4).\n",
    "\n",
    "---\n",
    "\n",
    "Note that *Clusters + Singles* refers to the number of clusters with two or more voxels and the number of additional individual voxels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/clusters.csv')\n",
    "\n",
    "table = ff.create_table(df2)\n",
    "\n",
    "plotly.offline.iplot(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Based on a quick look at the cluster numbers for each region it is not immediately clear that the lower regions are *more* contiguous than the higher level regions.\n",
    "\n",
    "It could also be the case that the lower regions just look more contiguous to they eye when sliced in AFNI as they are less *complexly* contiguous than the higher regions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise was to investigate how well pRFs of differing eccentricity might, on their own, serve to predict stimulus category. Our initial hypotheses were that categories such as faces and objects might be more accurately categorized by foveal regions, while places/scenes might benefit from peripheral vision, and therefore peripheral pRFs might be more accurate categorizers. While we did not find any significant evidence to support our hypotheses, there were trends that aligned with our premise. For example, in general, peripheral regions were more accurate at classifiying scenes, while for object and faces, the foveal pRFs of higher visual regions were slightly more predictive.   \n",
    "\n",
    "That said, our study suffered from two major problems. \n",
    "\n",
    "**Not enough subjects**\n",
    "\n",
    "With only three subjects, power and significance were bound to be hard to come by.\n",
    "\n",
    "**Too narrow a field of view**\n",
    "\n",
    "Due to the nature of the experimental setup, with the subject in a scanner looking at a screen a number of meters away, the maximum angle at which any stimuli could be viewed was well under 10 degrees. This is far below the true peripheral field of view for humans, and ignores large regions of visual receptivity. In order to truly measure the difference between foveal and peripheral eccentricities, and to do so in a more fine grained manner, it would be very helpful to have the stimuli cover a much larger range of the field of view.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dumoulin, S. O., & Wandell, B. A. (2008). Population receptive field estimates in human visual cortex. *NeuroImage*, 39(2), 647–660.\n",
    "\n",
    "Fairchild, M. D. (2013). *Color Appearance Models*. John Wiley & Sons.\n",
    "\n",
    "Fovea centralis. (n.d.). In *Wikipedia*. Retrieved April 7, 2018, from https://en.wikipedia.org/wiki/Fovea_centralis\n",
    "\n",
    "Peripheral vision. (n.d.). In *Wikipedia*. Retrieved April 7, 2018, from https://en.wikipedia.org/wiki/Peripheral_vision\n",
    "\n",
    "Smith, A. T., Singh, K. D., Williams, A. L., & Greenlee, M. W. (2001). Estimating receptive field size from fMRI data in human striate and extrastriate visual cortex. *Cerebral Cortex*, 11(12), 1182–1190.\n",
    "\n",
    "Victor, J. D., Purpura, K., Katz, E., & Mao, B. (1994). Population encoding of spatial frequency, orientation, and color in macaque V1. *Journal of Neurophysiology*, 72(5), 2151–2166."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
